profiling crime via scala:
spark-shell --deploy-mode client -i  FILENAME.scala
// Read clean_data.txt
val input_clean = sc.textFile("clean_data.txt")
val m_clean = input_clean.map(_.split(","))

// Read crime.csv
val input_crime = sc.textFile("crime.csv")
val m_crime = input_crime.map(line => {
  val arr = line.split(",") 
  val lat = arr(0).toDouble
  val lon = arr(1).replaceAll("\"", "").trim.toDouble
  (lat, lon)
})


// Initialize the SQLContext
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.Row
import org.apache.spark.sql.types._

val sqlContext = new SQLContext(sc)

// Create RDD for clean_data coordinates
val rowRDD_clean = m_clean.map(arr => (arr(0).toDouble, arr(1).toDouble))
val schema_clean = StructType(
  List(
    StructField("Latitude", DoubleType, nullable = true),
    StructField("Longitude", DoubleType, nullable = true)
  )
)

// Create RDD for crime coordinates
val rowRDD_crime = m_crime.map(arr => (arr._1, arr._2))
val schema_crime = StructType(
  List(
    StructField("Latitude", DoubleType, nullable = true),
    StructField("Longitude", DoubleType, nullable = true)
  )
)

import sqlContext.implicits._
val df_clean = rowRDD_clean.toDF("Latitude", "Longitude")
val df_crime = rowRDD_crime.toDF("Latitude", "Longitude")

// Calculate counts for crime coordinates near clean_data coordinates
val counts = df_clean.collect().map(row => {
  val coord = (row.getDouble(0), row.getDouble(1))
  val count = df_crime.rdd.filter(otherRow => {
    val otherCoord = (otherRow.getDouble(0), otherRow.getDouble(1))
    val distance = math.sqrt(math.pow(coord._1 - otherCoord._1, 2) + math.pow(coord._2 - otherCoord._2, 2))
    distance <= 0.025
  }).count()
  (coord, count)
})

val countsRDD = sc.parallelize(counts)
// Create a new DataFrame that combines coordinates with count
val countsDF = countsRDD.toDF("Coordinate", "Count")

val average = countsDF.agg(avg("Count")).collect()(0).getDouble(0)
val median = countsDF.stat.approxQuantile("Count", Array(0.5), 0.01)(0)

countsDF.show()
println(s"Average count: $average")
println(s"Median count: $median")

// Create a new DataFrame without the Coordinate column
val countsDF2 = countsDF.withColumn("Latitude", $"Coordinate._1").withColumn("Longitude", $"Coordinate._2").drop("Coordinate").sort($"Latitude")


countsDF2.coalesce(1).write.format("csv").option("header", "true").save("crimesortbylatitude.csv")




=============================

after using above three step in scala, i use hdfs dfs -get to get the cvs file, then zip it and download it.

i combined bikesortbylatitude and crimesortbylatitude column  to get bikecrimratio.cvs.